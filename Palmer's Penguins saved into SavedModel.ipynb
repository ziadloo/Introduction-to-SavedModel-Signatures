{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440cc87a-632a-43db-956f-f60c926fdc57",
   "metadata": {},
   "source": [
    "# Using SavedModel's signature to inlucde preprocessing and postprocessing\n",
    "\n",
    "This notebook contains the code for my article:\n",
    "\n",
    "[A Comprehensive Guide to Creating Keras Models including Preprocessing and Postprocessing for Seamless Deployment with TF-Serving using Signatures]()\n",
    "\n",
    "For an in-depth explanation, please refer to that link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fa100-0210-4f1b-ba09-b4b6f4cfe9cc",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3323ce3-b411-43dd-be0c-fe42007006c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912b8b8-255d-4c20-ae72-7708e9e2bec2",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1169aa-f9d9-4d9a-8cd7-bb737fb103ee",
   "metadata": {},
   "source": [
    "This part assumes you are using a Linux-compatible OS. If that is not the case, you can either change them to your specific needs to simply ignore them and do the same thing manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cec7a-03b0-42dc-855b-b18f27068280",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013f5d6-2398-4ccd-962f-4eb1812b60f8",
   "metadata": {},
   "source": [
    "The dataset contains a couple of records with NA values. We need to exclude these samples since they will mess up with the schema extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b091fe2-eb9a-4e40-a993-e4f199e427ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get rid of the record with NA in them\n",
    "!grep -Ev \"NA\" ./penguins.csv > ./penguins_filtered.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4960d5-33a4-4734-ad8b-609e50b8f35d",
   "metadata": {},
   "source": [
    "## Analysing the dataset\n",
    "\n",
    "Here, we'll be using the Tensorflow data validation library to analyse the dataset and learn more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73563d49-be44-4d44-994a-2de52d15722e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples_file = './penguins_filtered.csv'\n",
    "dataset_stats = tfdv.generate_statistics_from_csv(examples_file)\n",
    "tfdv.visualize_statistics(dataset_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b716f5-3a34-4f23-87b5-4ab3ea7d18b0",
   "metadata": {},
   "source": [
    "The same library can help us identify the type of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90ccb6-082c-4a7e-9099-f92a41e896e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(dataset_stats)\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69691bc-f1c0-486b-8c2d-dff2801fc412",
   "metadata": {},
   "source": [
    "## Loading and transforming the dataset for training\n",
    "\n",
    "Before learning how to transforming the dataset the proper way, let's do it in a simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319b652-06c8-4638-bb2a-5c2d613b6ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguins_ds = tf.data.experimental.make_csv_dataset(\n",
    "    examples_file,\n",
    "    batch_size=5,\n",
    "    label_name='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99f24a-8bbb-4857-b63b-b6e9123778ee",
   "metadata": {},
   "source": [
    "Peeking into the orignal dataset to see some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b6bee-6914-44f9-838d-3fe385faadee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for element in penguins_ds.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b08e8-7071-452f-b720-d0d4451f6474",
   "metadata": {},
   "source": [
    "Applying the transformation to the dataset directly, using the \"map\" method.\n",
    "\n",
    "Out of the 7 input features, 6 of them are one-hot encoded and one is normalized. The label feature is also one-hot encoded.\n",
    "\n",
    "The boundaries for the discretization is calculated using the statistical analysis done above. For all one-hot encodings, 10 buckets are assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c66a30-0a5b-4b71-b091-b309863ed786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "island_lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                tf.constant(['Biscoe', 'Dream', 'Torgersen']),\n",
    "                tf.range(start=0, limit=3, delta=1, dtype=tf.int32)),\n",
    "            default_value=0)\n",
    "\n",
    "sex_lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                tf.constant(['female', 'male']),\n",
    "                tf.range(start=0, limit=2, delta=1, dtype=tf.int32)),\n",
    "            default_value=0)\n",
    "\n",
    "species_lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                tf.constant(['Adelie', 'Chinstrap', 'Gentoo']),\n",
    "                tf.range(start=0, limit=3, delta=1, dtype=tf.int32)),\n",
    "            default_value=0)\n",
    "\n",
    "def calc_bins(start, end, count):\n",
    "    step = (end - start) / (count)\n",
    "    return list(np.linspace(start+step, end, count - 1, endpoint=False))\n",
    "\n",
    "bill_length_mapper = layers.Discretization(bin_boundaries=calc_bins(32.1, 59.6, 10),\n",
    "                                           output_mode='one_hot')\n",
    "flapper_length_mapper = layers.Discretization(bin_boundaries=calc_bins(172.0, 231.0, 10),\n",
    "                                              output_mode='one_hot')\n",
    "body_mass_mapper = layers.Discretization(bin_boundaries=calc_bins(2700.0, 6300.0, 10),\n",
    "                                         output_mode='one_hot')\n",
    "\n",
    "\n",
    "def one_hot_encode(feature, depth):\n",
    "    feature = tf.squeeze(feature)\n",
    "    return tf.one_hot(feature, depth=depth)\n",
    "\n",
    "def transform(features, label):\n",
    "    return ({'island': one_hot_encode(island_lookup.lookup(features['island']), 3),\n",
    "             'sex': one_hot_encode(sex_lookup.lookup(features['sex']), 2),\n",
    "             'bill_length_mm': bill_length_mapper(features['bill_length_mm']),\n",
    "             'bill_depth_mm': (features['bill_depth_mm'] - 17.16) / 1.97,\n",
    "             'flipper_length_mm': flapper_length_mapper(features['flipper_length_mm']),\n",
    "             'body_mass_g': body_mass_mapper(features['body_mass_g']),\n",
    "             'year': one_hot_encode(features['year'] - 2007, 3)},\n",
    "            one_hot_encode(species_lookup.lookup(label), 3))\n",
    "\n",
    "preprocessed_ds = penguins_ds.map(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe54011-3015-49da-8664-ef554ddee97a",
   "metadata": {},
   "source": [
    "Let's print and see the result of the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124cd8d-02d1-4fc5-b90c-50aeabe4d9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for element in preprocessed_ds.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab5cd9-ecbd-495b-9106-53ca01605172",
   "metadata": {},
   "source": [
    "## Designing and training a Keras model\n",
    "\n",
    "Here, we are instantiating a Keras model using functional API since we have multiple inputs. The design of the model is not really important since this notebook aims at preprocessing and postprocessing, and not trainging a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed3ad5-3a55-4d48-a695-d2f5d776685b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input layers\n",
    "island_input = Input(batch_shape=(None, 3), name='island')\n",
    "sex_input = Input(batch_shape=(None, 2), name='sex')\n",
    "bill_length_mm_input = Input(batch_shape=(None, 10), name='bill_length_mm')\n",
    "bill_depth_mm_input = Input(batch_shape=(None, 1), name='bill_depth_mm')\n",
    "flipper_length_mm_input = Input(batch_shape=(None, 10), name='flipper_length_mm')\n",
    "body_mass_g_input = Input(batch_shape=(None, 10), name='body_mass_g')\n",
    "year_input = Input(batch_shape=(None, 3), name='year')\n",
    "\n",
    "# Stack the input layers on top of each other\n",
    "input_layers = layers.concatenate([island_input,\n",
    "                                   sex_input,\n",
    "                                   bill_length_mm_input,\n",
    "                                   bill_depth_mm_input,\n",
    "                                   flipper_length_mm_input,\n",
    "                                   body_mass_g_input, year_input])\n",
    "\n",
    "# Actual learning layers\n",
    "x = layers.Dense(128, activation='relu')(input_layers)\n",
    "output = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = models.Model([island_input,\n",
    "                      sex_input,\n",
    "                      bill_length_mm_input,\n",
    "                      bill_depth_mm_input,\n",
    "                      flipper_length_mm_input,\n",
    "                      body_mass_g_input, year_input],\n",
    "                     output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926437fa-ba02-4ab6-b862-c724afc6bf92",
   "metadata": {},
   "source": [
    "Traning the model. This model is an overkill for this task and as the result, the trained model is overfit and cannot generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec5d0f-4d0c-48a9-a2aa-4cd7a9ca1feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(preprocessed_ds, epochs=10, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a2e1d-816b-4182-99a4-1657bfe14f34",
   "metadata": {},
   "source": [
    "Let's test the trained model and see the generated predictions.\n",
    "\n",
    "The important note here is that we have to use the preprocessed dataset or the model will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf1a9e-4c7b-4e72-9d3a-172f20b8edc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for example in preprocessed_ds.take(1):\n",
    "    print('\\nInput tensors:\\n\\n', example[0])\n",
    "    result = model(example[0])\n",
    "    print('\\nOutput tensors:\\n\\n', example[1])\n",
    "    print('\\nComparing output and target:\\n')\n",
    "    print('Output: ', tf.argmax(result, axis=-1))\n",
    "    print('Target: ', tf.argmax(example[1], axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7a581-8145-4984-9c6f-21a46b49c38c",
   "metadata": {},
   "source": [
    "## Adding the preprocessing and postprocessing layers and saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448851ec-3bb4-4d06-89fa-f457587e8846",
   "metadata": {},
   "source": [
    "This class will transform the categorical features into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16653507-be77-4b5a-9e77-85bfacba053c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class OneHotEncoder(layers.Layer):\n",
    "    def __init__(self, inputs: list, **kwargs):\n",
    "        super(OneHotEncoder, self).__init__(**kwargs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mapping_lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                tf.constant(self.inputs),\n",
    "                tf.range(start=0, limit=len(self.inputs), delta=1, dtype=tf.int32)),\n",
    "            default_value=0)\n",
    "\n",
    "    def call(self, input):\n",
    "        mapped = self.mapping_lookup.lookup(input)\n",
    "        one_hot = tf.one_hot(mapped, depth=len(self.inputs))\n",
    "        return tf.reshape(one_hot, [-1, len(self.inputs)])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['inputs'] = self.inputs\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c886af0-36d0-4226-9ad9-9cfe1747a649",
   "metadata": {},
   "source": [
    "This class is designed to transform the numberical features into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713cc43-5e86-4ce7-8b4d-de4e44587523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class OneHotDiscretization(layers.Layer):\n",
    "    def __init__(self, start, end, count, **kwargs):\n",
    "        super(OneHotDiscretization, self).__init__(**kwargs)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.count = count\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bin_boundaries = self.calc_bins(self.start, self.end, self.count)\n",
    "        self.disc = tf.keras.layers.Discretization(bin_boundaries=self.bin_boundaries,\n",
    "                                                   output_mode='one_hot')\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.disc(input)\n",
    "\n",
    "    def calc_bins(self, start, end, count):\n",
    "        step = (end - start) / (count)\n",
    "        return list(np.linspace(start+step, end, count - 1, endpoint=False))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['start'] = self.start\n",
    "        config['end'] = self.end\n",
    "        config['count'] = self.count\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adeedb3-c991-40b9-b6a9-a2284e136daa",
   "metadata": {},
   "source": [
    "This class will take in one-hot encoding and transform them into original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84a30a-6ac6-4fcc-ae6e-66e4f753364f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class OneHotDecoder(layers.Layer):\n",
    "    def __init__(self, outputs: list, **kwargs):\n",
    "        super(OneHotDecoder, self).__init__(**kwargs)\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mapping_lookup = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(\n",
    "                tf.range(start=0, limit=len(self.outputs), delta=1, dtype=tf.int32),\n",
    "                tf.constant(self.outputs)),\n",
    "            default_value=self.outputs[0])\n",
    "\n",
    "    def call(self, one_hot):\n",
    "        index = tf.math.argmax(one_hot, axis=-1)\n",
    "        prediction = self.mapping_lookup.lookup(tf.cast(index, dtype=tf.int32))\n",
    "        return prediction\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['outputs'] = self.outputs\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aed490-2d32-4567-a0a0-1c1ca7164ec6",
   "metadata": {},
   "source": [
    "These three functions will create a signature which add preprocessing and postprocessing to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c784e0-efa2-4429-a740-a8cda5431c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn():\n",
    "    \"\"\"Transforming the dataset's structure to model's expectation\"\"\"\n",
    "\n",
    "    island_input = Input(shape=(1,),\n",
    "                         name='island',\n",
    "                         dtype=tf.string)\n",
    "    island_encoder = OneHotEncoder(['Biscoe',\n",
    "                                    'Dream',\n",
    "                                    'Torgersen'])(island_input)\n",
    "\n",
    "    sex_input = Input(shape=(1,),\n",
    "                      name='sex',\n",
    "                      dtype=tf.string)\n",
    "    sex_encoder = OneHotEncoder(['female',\n",
    "                                 'male'])(sex_input)\n",
    "\n",
    "    bill_length_input = Input(shape=(1,),\n",
    "                              name='bill_length_mm',\n",
    "                              dtype=tf.float32)\n",
    "    bill_length_mapper = OneHotDiscretization(32.1,\n",
    "                                              59.6,\n",
    "                                              10)(bill_length_input)\n",
    "    \n",
    "    bill_depth_input = Input(shape=(1,),\n",
    "                             name='bill_depth_mm',\n",
    "                             dtype=tf.float32)\n",
    "    bill_depth_mapper = (bill_depth_input - 17.16) / 1.97\n",
    "    \n",
    "    flipper_length_input = Input(shape=(1,),\n",
    "                                 name='flipper_length_mm',\n",
    "                                 dtype=tf.int32)\n",
    "    flipper_length_mapper = OneHotDiscretization(172.0,\n",
    "                                                 231.0,\n",
    "                                                 10)(flipper_length_input)\n",
    "    \n",
    "    body_mass_input = Input(shape=(1,),\n",
    "                            name='body_mass_g',\n",
    "                            dtype=tf.int32)\n",
    "    body_mass_mapper = OneHotDiscretization(2700.0,\n",
    "                                            6300.0,\n",
    "                                            10)(body_mass_input)\n",
    "    \n",
    "    year_input = Input(shape=(1,),\n",
    "                       name='year',\n",
    "                       dtype=tf.int32)\n",
    "    year_mapper = tf.reshape(tf.one_hot(year_input - 2007,\n",
    "                                        depth=3), [-1, 3])\n",
    "\n",
    "    return models.Model([island_input,\n",
    "                         sex_input,\n",
    "                         bill_length_input,\n",
    "                         bill_depth_input,\n",
    "                         flipper_length_input,\n",
    "                         body_mass_input,\n",
    "                         year_input],\n",
    "                        [island_encoder,\n",
    "                         sex_encoder,\n",
    "                         bill_length_mapper,\n",
    "                         bill_depth_mapper,\n",
    "                         flipper_length_mapper,\n",
    "                         body_mass_mapper,\n",
    "                         year_mapper])\n",
    "\n",
    "\n",
    "def postprocessing_fn():\n",
    "    species_input = Input(shape=(3,), name='species', dtype=tf.float32)\n",
    "    species_lookup = OneHotDecoder(['Adelie',\n",
    "                                    'Chinstrap',\n",
    "                                    'Gentoo'])(species_input)\n",
    "    \n",
    "    return models.Model([species_input], [species_lookup])\n",
    "\n",
    "\n",
    "def serving_default_fn(model):\n",
    "    model.preprocessing_layer = preprocessing_fn()\n",
    "    model.postprocessing_layer = postprocessing_fn()\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.string,\n",
    "                                                name='island'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.string,\n",
    "                                                name='sex'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.float32,\n",
    "                                                name='bill_length_mm'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.float32,\n",
    "                                                name='bill_depth_mm'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.int32,\n",
    "                                                name='flipper_length_mm'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.int32,\n",
    "                                                name='body_mass_g'),\n",
    "                                  tf.TensorSpec(shape=[None,],\n",
    "                                                dtype=tf.int32,\n",
    "                                                name='year')])\n",
    "    def _serving_default_fn(island, sex,\n",
    "                            bill_length_mm,\n",
    "                            bill_depth_mm,\n",
    "                            flipper_length_mm,\n",
    "                            body_mass_g,\n",
    "                            year):\n",
    "        \"\"\"Composing an end-to-end model, including preprocessing and postprocessing\"\"\"\n",
    "\n",
    "        inputs = {'island': island,\n",
    "                  'sex': sex,\n",
    "                  'bill_length_mm': bill_length_mm,\n",
    "                  'bill_depth_mm': bill_depth_mm,\n",
    "                  'flipper_length_mm': flipper_length_mm,\n",
    "                  'body_mass_g': body_mass_g,\n",
    "                  'year': year}\n",
    "        \n",
    "        transformed = model.preprocessing_layer(inputs)\n",
    "        output = model(transformed)\n",
    "        return model.postprocessing_layer(output)\n",
    "    \n",
    "    return _serving_default_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e74495-daad-4709-a97b-b6b7d5f501ba",
   "metadata": {},
   "source": [
    "Saving the model and a signature which includes the proprocessing and postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869caa6-8e11-44c7-9e67-5d7a3a82fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('palmer_penguins', save_format='tf', signatures={'serving_default': serving_default_fn(model)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2a842-5781-4bd4-8e11-cffde87b1e84",
   "metadata": {},
   "source": [
    "Demonstrating how the three layers can work together to transform the features, generate predictions, and finally, transoform the output to human-readable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbceb5-0b12-4c0d-aa5d-8e651bae05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in penguins_ds.take(1):\n",
    "    transformed = model.preprocessing_layer(example[0])\n",
    "    output = model(transformed)\n",
    "    result = model.postprocessing_layer(output)\n",
    "    print('\\nOutput:\\n')\n",
    "    print(result)\n",
    "    print('\\nTarget:\\n')\n",
    "    print(example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef467796-4edb-4207-bdbf-cab524e3c5d2",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "\n",
    "Loading the saved model into a new object for another demonstration.\n",
    "\n",
    "At this point, you can restart the kernel and see that model definition is not needed for the following code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fe90b-daec-4074-9cfe-9855d939e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"palmer_penguins\")\n",
    "list(reconstructed_model.signatures.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3053c4-ea58-48fb-b571-b302440a2cf5",
   "metadata": {},
   "source": [
    "Using the signature for inference.\n",
    "\n",
    "The signature cannot be called with positional arguments, it needs to be called by named arguments. And this is by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28740f-7765-413b-b521-96d8c970e52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sig = reconstructed_model.signatures['serving_default']\n",
    "\n",
    "for example in penguins_ds.take(1):\n",
    "    result = sig(island = example[0]['island'],\n",
    "                 sex = example[0]['sex'],\n",
    "                 bill_length_mm = example[0]['bill_length_mm'],\n",
    "                 bill_depth_mm = example[0]['bill_depth_mm'],\n",
    "                 flipper_length_mm = example[0]['flipper_length_mm'],\n",
    "                 body_mass_g = example[0]['body_mass_g'],\n",
    "                 year = example[0]['year'])\n",
    "    print('\\nInput:\\n\\n', example[0])\n",
    "    print('\\nOutput:\\n\\n', result)\n",
    "    print('\\nTarget:\\n\\n', example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784477ba-f940-4074-862a-85adc0297906",
   "metadata": {},
   "source": [
    "## Deploying the model to TF-Serving\n",
    "\n",
    "In order to feed the saved model to a TF-Serving instance, we need change the folder structure first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018a93f-59f4-44be-a8a8-87373f775ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir ./palmer_penguins/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63e807-c29a-4f27-96bb-db57f6e48ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv ./palmer_penguins/assets \\\n",
    "    ./palmer_penguins/fingerprint.pb \\\n",
    "    ./palmer_penguins/keras_metadata.pb \\\n",
    "    ./palmer_penguins/saved_model.pb  \\\n",
    "    ./palmer_penguins/variables \\\n",
    "    ./palmer_penguins/2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a30370-bee3-455b-9c07-433a4346d9f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Starting a docker to run a TF-Serving instance. You might need to change the folder path.\n",
    "\n",
    "```shell\n",
    "$ docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"/home/mehran/penguins/palmer_penguins:/models/palmer_penguins\" \\\n",
    "    -e MODEL_NAME=palmer_penguins \\\n",
    "    --name \"palmer_penguins\" \\\n",
    "    tensorflow/serving\n",
    "```\n",
    "\n",
    "Then, you can send a request to the server like this:\n",
    "\n",
    "```shell\n",
    "$ curl --location --request POST 'http://127.0.0.1:8501/v1/models/palmer_penguins:predict' \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data-raw '{\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"island\": \"Dream\",\n",
    "            \"bill_length_mm\": 39.5,\n",
    "            \"bill_depth_mm\": 16.7,\n",
    "            \"flipper_length_mm\": 178,\n",
    "            \"body_mass_g\": 3250,\n",
    "            \"sex\": \"female\",\n",
    "            \"year\": 2007\n",
    "        },\n",
    "        {\n",
    "            \"island\": \"Biscoe\",\n",
    "            \"bill_length_mm\": 44.4,\n",
    "            \"bill_depth_mm\": 17.3,\n",
    "            \"flipper_length_mm\": 219,\n",
    "            \"body_mass_g\": 5250,\n",
    "            \"sex\": \"male\",\n",
    "            \"year\": 2008\n",
    "        }\n",
    "    ]\n",
    "}'\n",
    "```\n",
    "\n",
    "Which will output:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"predictions\": [\"Adelie\", \"Gentoo\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Once you are done with TF-Serving, you can stop it:\n",
    "\n",
    "```shell\n",
    "$ docker stop palmer_penguins\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
